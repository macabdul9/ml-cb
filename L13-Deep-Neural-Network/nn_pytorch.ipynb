{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn-pytorch.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW65vrBlyE33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F1W3BSZyPuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float) # 3 X 2 tensor\n",
        "y = torch.tensor(([92], [100], [89]), dtype=torch.float) # 3 X 1 tensor\n",
        "xPredicted = torch.tensor(([4, 8]), dtype=torch.float) # 1 X 2 tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wajb2hklyhyv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28f42484-a427-4626-d6f3-32780e50877f"
      },
      "source": [
        "print(X.size())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F-6zG_5z2_W",
        "colab_type": "text"
      },
      "source": [
        "### scaling what is also called normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZq9ueziznRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scale units\n",
        "X_max, _ = torch.max(X, 0)\n",
        "xPredicted_max, _ = torch.max(xPredicted, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho7i7j3sz1Yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.div(X, X_max)\n",
        "xPredicted = torch.div(xPredicted, xPredicted_max)\n",
        "y = y / 100  # max test score is 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1W1L2F80Ax4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d35addf5-f68e-4927-b20c-007748541fbb"
      },
      "source": [
        "X"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6667, 1.0000],\n",
              "        [0.3333, 0.5556],\n",
              "        [1.0000, 0.6667]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMNa-u6n0BgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtQ3jtZP0bb5",
        "colab_type": "text"
      },
      "source": [
        "### model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb8havnz0cv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neural_Network(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Neural_Network, self).__init__()\n",
        "        # parameters\n",
        "        # TODO: parameters can be parameterized instead of declaring them here\n",
        "        self.inputSize = 2\n",
        "        self.outputSize = 1\n",
        "        self.hiddenSize = 3\n",
        "        \n",
        "        # weights\n",
        "        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor\n",
        "        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n",
        "        \n",
        "    def forward(self, X):\n",
        "        self.z = torch.matmul(X, self.W1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
        "        self.z2 = self.sigmoid(self.z) # activation function\n",
        "        self.z3 = torch.matmul(self.z2, self.W2)\n",
        "        o = self.sigmoid(self.z3) # final activation function\n",
        "        return o\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1 + torch.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        # derivative of sigmoid\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "        self.o_error = y - o # error in output\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
        "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
        "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
        "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
        "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        # forward + backward pass for training\n",
        "        o = self.forward(X)\n",
        "        self.backward(X, y, o)\n",
        "        \n",
        "    def saveWeights(self, model):\n",
        "        # we will use the PyTorch internal storage functions\n",
        "        torch.save(model, \"NN\")\n",
        "        # you can reload model with all the weights and so forth with:\n",
        "        # torch.load(\"NN\")\n",
        "        \n",
        "    def predict(self):\n",
        "        print (\"Predicted data based on trained weights: \")\n",
        "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
        "        print (\"Output: \\n\" + str(self.forward(xPredicted)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kaFQVEp0dbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN = Neural_Network()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l18toaI80kIu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "680579c0-666a-4e6f-ec1f-e20a10c4b3e4"
      },
      "source": [
        "NN = Neural_Network()\n",
        "for i in range(100):  # trains the NN 1,000 times\n",
        "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
        "    NN.train(X, y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#0 Loss: 0.35289517045021057\n",
            "#1 Loss: 0.2923568785190582\n",
            "#2 Loss: 0.24119548499584198\n",
            "#3 Loss: 0.1988474279642105\n",
            "#4 Loss: 0.16405947506427765\n",
            "#5 Loss: 0.13558875024318695\n",
            "#6 Loss: 0.11237818002700806\n",
            "#7 Loss: 0.09353840351104736\n",
            "#8 Loss: 0.07830509543418884\n",
            "#9 Loss: 0.06601537019014359\n",
            "#10 Loss: 0.05610107257962227\n",
            "#11 Loss: 0.04808572307229042\n",
            "#12 Loss: 0.04157908260822296\n",
            "#13 Loss: 0.03626786544919014\n",
            "#14 Loss: 0.03190403804183006\n",
            "#15 Loss: 0.028293101117014885\n",
            "#16 Loss: 0.0252830907702446\n",
            "#17 Loss: 0.022755473852157593\n",
            "#18 Loss: 0.020617499947547913\n",
            "#19 Loss: 0.01879645697772503\n",
            "#20 Loss: 0.01723496802151203\n",
            "#21 Loss: 0.015887530520558357\n",
            "#22 Loss: 0.01471784058958292\n",
            "#23 Loss: 0.013696718029677868\n",
            "#24 Loss: 0.01280056033283472\n",
            "#25 Loss: 0.012010167352855206\n",
            "#26 Loss: 0.011309816502034664\n",
            "#27 Loss: 0.010686547495424747\n",
            "#28 Loss: 0.010129586793482304\n",
            "#29 Loss: 0.009629976935684681\n",
            "#30 Loss: 0.009180202148854733\n",
            "#31 Loss: 0.008773901499807835\n",
            "#32 Loss: 0.0084057142958045\n",
            "#33 Loss: 0.008071068674325943\n",
            "#34 Loss: 0.007766038179397583\n",
            "#35 Loss: 0.007487270515412092\n",
            "#36 Loss: 0.007231834810227156\n",
            "#37 Loss: 0.006997250020503998\n",
            "#38 Loss: 0.006781305652111769\n",
            "#39 Loss: 0.006582106929272413\n",
            "#40 Loss: 0.006397968623787165\n",
            "#41 Loss: 0.00622743321582675\n",
            "#42 Loss: 0.00606920151039958\n",
            "#43 Loss: 0.0059221223928034306\n",
            "#44 Loss: 0.005785183981060982\n",
            "#45 Loss: 0.005657488945871592\n",
            "#46 Loss: 0.005538215860724449\n",
            "#47 Loss: 0.005426671355962753\n",
            "#48 Loss: 0.005322187673300505\n",
            "#49 Loss: 0.005224201828241348\n",
            "#50 Loss: 0.0051321787759661674\n",
            "#51 Loss: 0.005045656580477953\n",
            "#52 Loss: 0.004964207764714956\n",
            "#53 Loss: 0.004887447226792574\n",
            "#54 Loss: 0.004815032705664635\n",
            "#55 Loss: 0.0047466387040913105\n",
            "#56 Loss: 0.004681979771703482\n",
            "#57 Loss: 0.004620788618922234\n",
            "#58 Loss: 0.004562824964523315\n",
            "#59 Loss: 0.004507872276008129\n",
            "#60 Loss: 0.004455723334103823\n",
            "#61 Loss: 0.00440620444715023\n",
            "#62 Loss: 0.004359135404229164\n",
            "#63 Loss: 0.004314348101615906\n",
            "#64 Loss: 0.004271719604730606\n",
            "#65 Loss: 0.004231106955558062\n",
            "#66 Loss: 0.004192376974970102\n",
            "#67 Loss: 0.004155435133725405\n",
            "#68 Loss: 0.004120160359889269\n",
            "#69 Loss: 0.004086453467607498\n",
            "#70 Loss: 0.0040542371571063995\n",
            "#71 Loss: 0.004023417830467224\n",
            "#72 Loss: 0.003993912599980831\n",
            "#73 Loss: 0.00396565580740571\n",
            "#74 Loss: 0.003938576206564903\n",
            "#75 Loss: 0.003912612330168486\n",
            "#76 Loss: 0.0038877008482813835\n",
            "#77 Loss: 0.003863786580041051\n",
            "#78 Loss: 0.0038408159743994474\n",
            "#79 Loss: 0.0038187503814697266\n",
            "#80 Loss: 0.0037975262384861708\n",
            "#81 Loss: 0.0037771137431263924\n",
            "#82 Loss: 0.003757470054551959\n",
            "#83 Loss: 0.003738558618351817\n",
            "#84 Loss: 0.0037203386891633272\n",
            "#85 Loss: 0.003702781395986676\n",
            "#86 Loss: 0.0036858527455478907\n",
            "#87 Loss: 0.003669520141556859\n",
            "#88 Loss: 0.0036537644919008017\n",
            "#89 Loss: 0.0036385534331202507\n",
            "#90 Loss: 0.0036238564644008875\n",
            "#91 Loss: 0.0036096610128879547\n",
            "#92 Loss: 0.003595945658162236\n",
            "#93 Loss: 0.0035826738458126783\n",
            "#94 Loss: 0.003569838823750615\n",
            "#95 Loss: 0.003557417541742325\n",
            "#96 Loss: 0.0035453960299491882\n",
            "#97 Loss: 0.003533747745677829\n",
            "#98 Loss: 0.003522467566654086\n",
            "#99 Loss: 0.0035115359351038933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5C-VttE0tjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "4f6bbd48-9ba1-457c-a576-3f022b43c3a7"
      },
      "source": [
        "NN.saveWeights(NN)\n",
        "NN.predict()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted data based on trained weights: \n",
            "Input (scaled): \n",
            "tensor([0.5000, 1.0000])\n",
            "Output: \n",
            "tensor([0.9169])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Neural_Network. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFM-1CQS05-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}